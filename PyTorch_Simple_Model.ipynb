{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_Simple Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7l9YUx8APdsHrRvp/vAxv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shrey-Viradiya/PyTorch_for_DL/blob/master/PyTorch_Simple_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJaIUzGO0sFZ",
        "outputId": "452e6384-4d15-428f-a788-d99af9ce1d37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct  1 07:23:47 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqCF3a522zHD"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtqw9bEryeu4"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") \n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh-EcpXQ3p-x"
      },
      "source": [
        "X, y = load_boston(return_X_y=True)\n",
        "X, y = shuffle(X,y, random_state = 259)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdua_TsGyke5"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=259)\n",
        "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=259)\n",
        "\n",
        "X_train = torch.from_numpy(X_train).to(device, dtype=torch.float32)\n",
        "X_test = torch.from_numpy(X_test).to(device, dtype=torch.float32)\n",
        "X_valid = torch.from_numpy(X_valid).to(device, dtype=torch.float32)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "y_train = torch.squeeze(y_train).to(device, dtype=torch.float32)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "y_test = torch.squeeze(y_test).to(device, dtype=torch.float32)\n",
        "y_valid = torch.from_numpy(y_valid)\n",
        "y_valid = torch.squeeze(y_valid).to(device, dtype=torch.float32)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-kKnr6cu1El"
      },
      "source": [
        "## Creating a Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCJ4kBJ0uvpf"
      },
      "source": [
        "class BostonModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.f1 = torch.nn.Linear(13,512)\n",
        "        self.f2 = torch.nn.Linear(512,64)\n",
        "        self.f3 = torch.nn.Linear(64,4)\n",
        "        self.f4 = torch.nn.Linear(4,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.f1(x))\n",
        "        x = torch.nn.functional.relu(self.f2(x))\n",
        "        x = torch.nn.functional.relu(self.f3(x))\n",
        "        x = self.f4(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdy_vU4nwBqU",
        "outputId": "d6bc9987-a2dc-4890-c43c-477b47c5c64b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "bst = BostonModel()\n",
        "bst.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BostonModel(\n",
              "  (f1): Linear(in_features=13, out_features=512, bias=True)\n",
              "  (f2): Linear(in_features=512, out_features=64, bias=True)\n",
              "  (f3): Linear(in_features=64, out_features=4, bias=True)\n",
              "  (f4): Linear(in_features=4, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELVgomBEwFHL"
      },
      "source": [
        "## Optimizer and Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_E6LubTwD8e"
      },
      "source": [
        "optimizer = torch.optim.SGD(bst.parameters(), lr = 0.1)\n",
        "loss_fun = torch.nn.MSELoss()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNUKQLXKwZiV"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F-4WPm4wY8q",
        "outputId": "5b6d077a-6e84-4343-97e1-d8507c67c4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 200\n",
        "batch_size = 32\n",
        "steps_per_batch = X_train.shape[0] // batch_size\n",
        "\n",
        "bst.train()\n",
        "for epoch in range(epochs):\n",
        "    training_loss = 0.0\n",
        "    for step in range(steps_per_batch):\n",
        "        curr_bat = torch.randint(X_train.shape[0], (batch_size,))\n",
        "        data = X_train[curr_bat]\n",
        "        tar = y_train[curr_bat]\n",
        "        \n",
        "        y_pred = bst(data)\n",
        "        y_pred = torch.squeeze(y_pred)\n",
        "        loss = loss_fun(y_pred, tar)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        training_loss += loss.data.item()\n",
        "    training_loss /= steps_per_batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = bst(X_valid)\n",
        "        output = torch.squeeze(output)\n",
        "        loss = loss_fun(output, y_valid)\n",
        "        validation_loss = loss.data.item()\n",
        "    print(f'\\repoch: {epoch} \\t\\t Training loss: {training_loss:8.3f} \\t\\t Validation loss: {validation_loss:8.3f} \\t')\n",
        "    "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\repoch: 0 \t\t Training loss:  231.623 \t\t Validation loss:  114.963 \t\n",
            "\repoch: 1 \t\t Training loss:   61.669 \t\t Validation loss:  106.757 \t\n",
            "\repoch: 2 \t\t Training loss:   85.396 \t\t Validation loss:  105.882 \t\n",
            "\repoch: 3 \t\t Training loss:   71.169 \t\t Validation loss:  105.886 \t\n",
            "\repoch: 4 \t\t Training loss:   81.216 \t\t Validation loss:  107.473 \t\n",
            "\repoch: 5 \t\t Training loss:   72.324 \t\t Validation loss:  108.246 \t\n",
            "\repoch: 6 \t\t Training loss:   83.788 \t\t Validation loss:  105.674 \t\n",
            "\repoch: 7 \t\t Training loss:   75.437 \t\t Validation loss:  105.926 \t\n",
            "\repoch: 8 \t\t Training loss:   73.481 \t\t Validation loss:  105.553 \t\n",
            "\repoch: 9 \t\t Training loss:   87.607 \t\t Validation loss:  105.966 \t\n",
            "\repoch: 10 \t\t Training loss:   69.937 \t\t Validation loss:  108.368 \t\n",
            "\repoch: 11 \t\t Training loss:   83.650 \t\t Validation loss:  107.942 \t\n",
            "epoch: 12 \t\t Training loss:   85.712 \t\t Validation loss:  105.938 \t\n",
            "epoch: 13 \t\t Training loss:   60.278 \t\t Validation loss:  107.146 \t\n",
            "epoch: 14 \t\t Training loss:   79.396 \t\t Validation loss:  106.467 \t\n",
            "epoch: 15 \t\t Training loss:   69.219 \t\t Validation loss:  107.885 \t\n",
            "epoch: 16 \t\t Training loss:   77.451 \t\t Validation loss:  107.643 \t\n",
            "epoch: 17 \t\t Training loss:   79.613 \t\t Validation loss:  106.245 \t\n",
            "epoch: 18 \t\t Training loss:   86.542 \t\t Validation loss:  106.346 \t\n",
            "epoch: 19 \t\t Training loss:   95.311 \t\t Validation loss:  105.541 \t\n",
            "epoch: 20 \t\t Training loss:   70.097 \t\t Validation loss:  108.034 \t\n",
            "epoch: 21 \t\t Training loss:   78.607 \t\t Validation loss:  106.569 \t\n",
            "epoch: 22 \t\t Training loss:   93.718 \t\t Validation loss:  105.555 \t\n",
            "epoch: 23 \t\t Training loss:   75.251 \t\t Validation loss:  106.168 \t\n",
            "epoch: 24 \t\t Training loss:   81.088 \t\t Validation loss:  106.186 \t\n",
            "epoch: 25 \t\t Training loss:   69.952 \t\t Validation loss:  107.809 \t\n",
            "epoch: 26 \t\t Training loss:   75.936 \t\t Validation loss:  105.564 \t\n",
            "epoch: 27 \t\t Training loss:   81.833 \t\t Validation loss:  105.551 \t\n",
            "epoch: 28 \t\t Training loss:   96.998 \t\t Validation loss:  105.577 \t\n",
            "epoch: 29 \t\t Training loss:   86.174 \t\t Validation loss:  105.695 \t\n",
            "epoch: 30 \t\t Training loss:   86.567 \t\t Validation loss:  105.830 \t\n",
            "epoch: 31 \t\t Training loss:   76.646 \t\t Validation loss:  105.806 \t\n",
            "epoch: 32 \t\t Training loss:   79.428 \t\t Validation loss:  107.131 \t\n",
            "epoch: 33 \t\t Training loss:   83.442 \t\t Validation loss:  107.106 \t\n",
            "epoch: 34 \t\t Training loss:   97.023 \t\t Validation loss:  105.570 \t\n",
            "epoch: 35 \t\t Training loss:   84.333 \t\t Validation loss:  107.272 \t\n",
            "epoch: 36 \t\t Training loss:   77.198 \t\t Validation loss:  107.062 \t\n",
            "epoch: 37 \t\t Training loss:   79.244 \t\t Validation loss:  106.157 \t\n",
            "epoch: 38 \t\t Training loss:   68.065 \t\t Validation loss:  107.069 \t\n",
            "epoch: 39 \t\t Training loss:   68.941 \t\t Validation loss:  107.677 \t\n",
            "epoch: 40 \t\t Training loss:   75.577 \t\t Validation loss:  105.670 \t\n",
            "epoch: 41 \t\t Training loss:   71.056 \t\t Validation loss:  107.232 \t\n",
            "epoch: 42 \t\t Training loss:   86.357 \t\t Validation loss:  105.541 \t\n",
            "epoch: 43 \t\t Training loss:   71.132 \t\t Validation loss:  106.299 \t\n",
            "epoch: 44 \t\t Training loss:   83.573 \t\t Validation loss:  105.560 \t\n",
            "epoch: 45 \t\t Training loss:   66.885 \t\t Validation loss:  107.544 \t\n",
            "epoch: 46 \t\t Training loss:   89.282 \t\t Validation loss:  105.841 \t\n",
            "epoch: 47 \t\t Training loss:   78.997 \t\t Validation loss:  105.544 \t\n",
            "epoch: 48 \t\t Training loss:   73.605 \t\t Validation loss:  107.961 \t\n",
            "epoch: 49 \t\t Training loss:   73.406 \t\t Validation loss:  107.598 \t\n",
            "epoch: 50 \t\t Training loss:   91.387 \t\t Validation loss:  106.745 \t\n",
            "epoch: 51 \t\t Training loss:   51.925 \t\t Validation loss:  109.071 \t\n",
            "epoch: 52 \t\t Training loss:   84.485 \t\t Validation loss:  106.046 \t\n",
            "epoch: 53 \t\t Training loss:   67.491 \t\t Validation loss:  107.777 \t\n",
            "epoch: 54 \t\t Training loss:   79.706 \t\t Validation loss:  105.783 \t\n",
            "epoch: 55 \t\t Training loss:   84.161 \t\t Validation loss:  105.883 \t\n",
            "epoch: 56 \t\t Training loss:   93.379 \t\t Validation loss:  105.585 \t\n",
            "epoch: 57 \t\t Training loss:   90.822 \t\t Validation loss:  105.546 \t\n",
            "epoch: 58 \t\t Training loss:   83.244 \t\t Validation loss:  105.885 \t\n",
            "epoch: 59 \t\t Training loss:   80.069 \t\t Validation loss:  106.844 \t\n",
            "epoch: 60 \t\t Training loss:   78.572 \t\t Validation loss:  106.246 \t\n",
            "epoch: 61 \t\t Training loss:   83.766 \t\t Validation loss:  105.892 \t\n",
            "epoch: 62 \t\t Training loss:   75.928 \t\t Validation loss:  106.189 \t\n",
            "epoch: 63 \t\t Training loss:   62.083 \t\t Validation loss:  107.900 \t\n",
            "epoch: 64 \t\t Training loss:   59.916 \t\t Validation loss:  108.712 \t\n",
            "epoch: 65 \t\t Training loss:   60.911 \t\t Validation loss:  107.111 \t\n",
            "epoch: 66 \t\t Training loss:   86.520 \t\t Validation loss:  107.021 \t\n",
            "epoch: 67 \t\t Training loss:   82.278 \t\t Validation loss:  106.672 \t\n",
            "epoch: 68 \t\t Training loss:   78.882 \t\t Validation loss:  106.928 \t\n",
            "epoch: 69 \t\t Training loss:   82.236 \t\t Validation loss:  105.733 \t\n",
            "epoch: 70 \t\t Training loss:   83.110 \t\t Validation loss:  106.188 \t\n",
            "epoch: 71 \t\t Training loss:   73.979 \t\t Validation loss:  106.942 \t\n",
            "epoch: 72 \t\t Training loss:   80.207 \t\t Validation loss:  107.275 \t\n",
            "epoch: 73 \t\t Training loss:   84.729 \t\t Validation loss:  106.773 \t\n",
            "epoch: 74 \t\t Training loss:   65.332 \t\t Validation loss:  107.085 \t\n",
            "epoch: 75 \t\t Training loss:   75.119 \t\t Validation loss:  107.045 \t\n",
            "epoch: 76 \t\t Training loss:   75.090 \t\t Validation loss:  108.814 \t\n",
            "epoch: 77 \t\t Training loss:   72.227 \t\t Validation loss:  105.745 \t\n",
            "epoch: 78 \t\t Training loss:   77.077 \t\t Validation loss:  106.566 \t\n",
            "epoch: 79 \t\t Training loss:   79.441 \t\t Validation loss:  107.057 \t\n",
            "epoch: 80 \t\t Training loss:   78.310 \t\t Validation loss:  105.788 \t\n",
            "epoch: 81 \t\t Training loss:   80.449 \t\t Validation loss:  106.023 \t\n",
            "epoch: 82 \t\t Training loss:   98.456 \t\t Validation loss:  106.391 \t\n",
            "epoch: 83 \t\t Training loss:   78.609 \t\t Validation loss:  107.067 \t\n",
            "epoch: 84 \t\t Training loss:   68.350 \t\t Validation loss:  106.092 \t\n",
            "epoch: 85 \t\t Training loss:   99.118 \t\t Validation loss:  105.587 \t\n",
            "epoch: 86 \t\t Training loss:   90.816 \t\t Validation loss:  106.009 \t\n",
            "epoch: 87 \t\t Training loss:   71.373 \t\t Validation loss:  107.028 \t\n",
            "epoch: 88 \t\t Training loss:   88.942 \t\t Validation loss:  105.977 \t\n",
            "epoch: 89 \t\t Training loss:   83.565 \t\t Validation loss:  105.630 \t\n",
            "epoch: 90 \t\t Training loss:   84.021 \t\t Validation loss:  106.767 \t\n",
            "epoch: 91 \t\t Training loss:   95.595 \t\t Validation loss:  105.547 \t\n",
            "epoch: 92 \t\t Training loss:   87.168 \t\t Validation loss:  106.144 \t\n",
            "epoch: 93 \t\t Training loss:   71.379 \t\t Validation loss:  107.797 \t\n",
            "epoch: 94 \t\t Training loss:   82.436 \t\t Validation loss:  106.825 \t\n",
            "epoch: 95 \t\t Training loss:   73.152 \t\t Validation loss:  106.152 \t\n",
            "epoch: 96 \t\t Training loss:   85.842 \t\t Validation loss:  105.723 \t\n",
            "epoch: 97 \t\t Training loss:   84.887 \t\t Validation loss:  106.619 \t\n",
            "epoch: 98 \t\t Training loss:   77.743 \t\t Validation loss:  106.654 \t\n",
            "epoch: 99 \t\t Training loss:   79.936 \t\t Validation loss:  107.993 \t\n",
            "epoch: 100 \t\t Training loss:   76.237 \t\t Validation loss:  106.772 \t\n",
            "epoch: 101 \t\t Training loss:   89.097 \t\t Validation loss:  108.033 \t\n",
            "epoch: 102 \t\t Training loss:   69.683 \t\t Validation loss:  108.027 \t\n",
            "epoch: 103 \t\t Training loss:   80.226 \t\t Validation loss:  105.648 \t\n",
            "epoch: 104 \t\t Training loss:   76.354 \t\t Validation loss:  106.520 \t\n",
            "epoch: 105 \t\t Training loss:  100.540 \t\t Validation loss:  105.604 \t\n",
            "epoch: 106 \t\t Training loss:   68.594 \t\t Validation loss:  106.307 \t\n",
            "epoch: 107 \t\t Training loss:   68.713 \t\t Validation loss:  108.639 \t\n",
            "epoch: 108 \t\t Training loss:   79.789 \t\t Validation loss:  105.729 \t\n",
            "epoch: 109 \t\t Training loss:   66.373 \t\t Validation loss:  105.955 \t\n",
            "epoch: 110 \t\t Training loss:   73.765 \t\t Validation loss:  106.500 \t\n",
            "epoch: 111 \t\t Training loss:   79.482 \t\t Validation loss:  105.896 \t\n",
            "epoch: 112 \t\t Training loss:   90.006 \t\t Validation loss:  106.579 \t\n",
            "epoch: 113 \t\t Training loss:   64.151 \t\t Validation loss:  109.552 \t\n",
            "epoch: 114 \t\t Training loss:   68.348 \t\t Validation loss:  107.706 \t\n",
            "epoch: 115 \t\t Training loss:   78.230 \t\t Validation loss:  106.828 \t\n",
            "epoch: 116 \t\t Training loss:   68.799 \t\t Validation loss:  108.382 \t\n",
            "epoch: 117 \t\t Training loss:   97.184 \t\t Validation loss:  106.285 \t\n",
            "epoch: 118 \t\t Training loss:   72.944 \t\t Validation loss:  107.975 \t\n",
            "epoch: 119 \t\t Training loss:   78.359 \t\t Validation loss:  107.482 \t\n",
            "epoch: 120 \t\t Training loss:   62.115 \t\t Validation loss:  109.953 \t\n",
            "epoch: 121 \t\t Training loss:   68.673 \t\t Validation loss:  106.925 \t\n",
            "epoch: 122 \t\t Training loss:   80.821 \t\t Validation loss:  107.451 \t\n",
            "epoch: 123 \t\t Training loss:   78.722 \t\t Validation loss:  107.417 \t\n",
            "epoch: 124 \t\t Training loss:   78.697 \t\t Validation loss:  105.601 \t\n",
            "epoch: 125 \t\t Training loss:   65.244 \t\t Validation loss:  107.754 \t\n",
            "epoch: 126 \t\t Training loss:   73.073 \t\t Validation loss:  106.327 \t\n",
            "epoch: 127 \t\t Training loss:   79.421 \t\t Validation loss:  105.910 \t\n",
            "epoch: 128 \t\t Training loss:   77.047 \t\t Validation loss:  106.247 \t\n",
            "epoch: 129 \t\t Training loss:   75.031 \t\t Validation loss:  105.776 \t\n",
            "epoch: 130 \t\t Training loss:   81.515 \t\t Validation loss:  106.429 \t\n",
            "epoch: 131 \t\t Training loss:   81.953 \t\t Validation loss:  105.947 \t\n",
            "epoch: 132 \t\t Training loss:   76.472 \t\t Validation loss:  106.077 \t\n",
            "epoch: 133 \t\t Training loss:   82.945 \t\t Validation loss:  106.499 \t\n",
            "epoch: 134 \t\t Training loss:   71.583 \t\t Validation loss:  105.925 \t\n",
            "epoch: 135 \t\t Training loss:   69.877 \t\t Validation loss:  106.190 \t\n",
            "epoch: 136 \t\t Training loss:   69.976 \t\t Validation loss:  106.008 \t\n",
            "epoch: 137 \t\t Training loss:   62.200 \t\t Validation loss:  106.002 \t\n",
            "epoch: 138 \t\t Training loss:   87.401 \t\t Validation loss:  106.003 \t\n",
            "epoch: 139 \t\t Training loss:   73.327 \t\t Validation loss:  107.250 \t\n",
            "epoch: 140 \t\t Training loss:   84.137 \t\t Validation loss:  106.047 \t\n",
            "epoch: 141 \t\t Training loss:   82.256 \t\t Validation loss:  106.616 \t\n",
            "epoch: 142 \t\t Training loss:   92.394 \t\t Validation loss:  105.939 \t\n",
            "epoch: 143 \t\t Training loss:   90.364 \t\t Validation loss:  105.589 \t\n",
            "epoch: 144 \t\t Training loss:   79.553 \t\t Validation loss:  106.745 \t\n",
            "epoch: 145 \t\t Training loss:   69.583 \t\t Validation loss:  105.622 \t\n",
            "epoch: 146 \t\t Training loss:   81.128 \t\t Validation loss:  105.645 \t\n",
            "epoch: 147 \t\t Training loss:   84.270 \t\t Validation loss:  105.929 \t\n",
            "epoch: 148 \t\t Training loss:   81.477 \t\t Validation loss:  105.638 \t\n",
            "epoch: 149 \t\t Training loss:   77.339 \t\t Validation loss:  105.754 \t\n",
            "epoch: 150 \t\t Training loss:   75.231 \t\t Validation loss:  106.238 \t\n",
            "epoch: 151 \t\t Training loss:   96.666 \t\t Validation loss:  105.574 \t\n",
            "epoch: 152 \t\t Training loss:   89.430 \t\t Validation loss:  105.619 \t\n",
            "epoch: 153 \t\t Training loss:   67.678 \t\t Validation loss:  108.683 \t\n",
            "epoch: 154 \t\t Training loss:   68.219 \t\t Validation loss:  108.997 \t\n",
            "epoch: 155 \t\t Training loss:   71.877 \t\t Validation loss:  108.958 \t\n",
            "epoch: 156 \t\t Training loss:   81.792 \t\t Validation loss:  106.584 \t\n",
            "epoch: 157 \t\t Training loss:   86.610 \t\t Validation loss:  105.586 \t\n",
            "epoch: 158 \t\t Training loss:   82.678 \t\t Validation loss:  106.665 \t\n",
            "epoch: 159 \t\t Training loss:   94.589 \t\t Validation loss:  105.583 \t\n",
            "epoch: 160 \t\t Training loss:   69.781 \t\t Validation loss:  106.659 \t\n",
            "epoch: 161 \t\t Training loss:   72.402 \t\t Validation loss:  106.748 \t\n",
            "epoch: 162 \t\t Training loss:   81.370 \t\t Validation loss:  105.718 \t\n",
            "epoch: 163 \t\t Training loss:   74.150 \t\t Validation loss:  105.891 \t\n",
            "epoch: 164 \t\t Training loss:   77.180 \t\t Validation loss:  105.762 \t\n",
            "epoch: 165 \t\t Training loss:   77.355 \t\t Validation loss:  106.283 \t\n",
            "epoch: 166 \t\t Training loss:   71.573 \t\t Validation loss:  107.455 \t\n",
            "epoch: 167 \t\t Training loss:   79.132 \t\t Validation loss:  106.493 \t\n",
            "epoch: 168 \t\t Training loss:   88.881 \t\t Validation loss:  105.918 \t\n",
            "epoch: 169 \t\t Training loss:   91.382 \t\t Validation loss:  105.864 \t\n",
            "epoch: 170 \t\t Training loss:   70.539 \t\t Validation loss:  108.451 \t\n",
            "epoch: 171 \t\t Training loss:   90.026 \t\t Validation loss:  105.554 \t\n",
            "epoch: 172 \t\t Training loss:   75.998 \t\t Validation loss:  106.266 \t\n",
            "epoch: 173 \t\t Training loss:   73.564 \t\t Validation loss:  106.094 \t\n",
            "epoch: 174 \t\t Training loss:   74.119 \t\t Validation loss:  107.673 \t\n",
            "epoch: 175 \t\t Training loss:   77.230 \t\t Validation loss:  105.547 \t\n",
            "epoch: 176 \t\t Training loss:   89.833 \t\t Validation loss:  105.596 \t\n",
            "epoch: 177 \t\t Training loss:   95.882 \t\t Validation loss:  105.866 \t\n",
            "epoch: 178 \t\t Training loss:   74.134 \t\t Validation loss:  106.334 \t\n",
            "epoch: 179 \t\t Training loss:   73.032 \t\t Validation loss:  106.384 \t\n",
            "epoch: 180 \t\t Training loss:   83.921 \t\t Validation loss:  107.997 \t\n",
            "epoch: 181 \t\t Training loss:   85.370 \t\t Validation loss:  106.923 \t\n",
            "epoch: 182 \t\t Training loss:   74.312 \t\t Validation loss:  106.928 \t\n",
            "epoch: 183 \t\t Training loss:   70.561 \t\t Validation loss:  106.311 \t\n",
            "epoch: 184 \t\t Training loss:   70.621 \t\t Validation loss:  106.468 \t\n",
            "epoch: 185 \t\t Training loss:   84.695 \t\t Validation loss:  105.756 \t\n",
            "epoch: 186 \t\t Training loss:   88.115 \t\t Validation loss:  105.542 \t\n",
            "epoch: 187 \t\t Training loss:   77.818 \t\t Validation loss:  108.375 \t\n",
            "epoch: 188 \t\t Training loss:   78.402 \t\t Validation loss:  105.836 \t\n",
            "epoch: 189 \t\t Training loss:   71.326 \t\t Validation loss:  106.839 \t\n",
            "epoch: 190 \t\t Training loss:   56.018 \t\t Validation loss:  109.495 \t\n",
            "epoch: 191 \t\t Training loss:   69.050 \t\t Validation loss:  106.743 \t\n",
            "epoch: 192 \t\t Training loss:   79.334 \t\t Validation loss:  105.575 \t\n",
            "epoch: 193 \t\t Training loss:   80.474 \t\t Validation loss:  106.259 \t\n",
            "epoch: 194 \t\t Training loss:   77.373 \t\t Validation loss:  107.614 \t\n",
            "epoch: 195 \t\t Training loss:   76.865 \t\t Validation loss:  108.006 \t\n",
            "epoch: 196 \t\t Training loss:   71.651 \t\t Validation loss:  105.840 \t\n",
            "epoch: 197 \t\t Training loss:   78.031 \t\t Validation loss:  106.487 \t\n",
            "epoch: 198 \t\t Training loss:   70.721 \t\t Validation loss:  106.590 \t\n",
            "epoch: 199 \t\t Training loss:   81.425 \t\t Validation loss:  105.565 \t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gVu5aBr2Xm9",
        "outputId": "a1728754-fd02-459d-b694-62e040271d40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with torch.no_grad():\n",
        "    output = bst(X_test)\n",
        "    output = torch.squeeze(output)\n",
        "    loss = loss_fun(output, y_test)\n",
        "    test_loss = loss.data.item()\n",
        "\n",
        "print(f'Test loss: {test_loss:.3f}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 84.474\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}